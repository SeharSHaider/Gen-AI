{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA2.2 Part A - Hidden Markov Model (HMM) Based Part Of Speech Tagging\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will be making a Part of Speech (POS) Tagger using an HMM via Supervised Matrix/Parameter Initialization and Viterbi Decoding.\n",
    "\n",
    "Hidden Markov Model (HMM) is a statistical model used to describe a system that transitions between a set of hidden states over time, generating observable outcomes.\n",
    "\n",
    "In the context of Part-of-Speech (POS) tagging, HMMs are used to model the sequence of words in a sentence as a sequence of hidden states representing the POS tags. The observable outcomes are the actual words in the sentence.\n",
    "\n",
    "## Terminology\n",
    "\n",
    "__Supervised Matrix/Parameter Initialization__: involves setting the initial values of the parameters in the model, specifically the emission probabilities matrix and the transition probabilities matrix. In the case of supervised learning tasks, where the HMM is trained on annotated data (e.g., for Part-of-Speech tagging), the initialization involves estimating the initial values of these matrices based on the observed training data. This initialization is crucial as it provides the starting point for the model to learn and adjust its parameters during the training process. The matrices are typically initialized using statistical information derived from the frequency of transitions and emissions observed in the training dataset.\n",
    "\n",
    "__Viterbi Decoding__: is a dynamic programming algorithm used for finding the most likely sequence of hidden states (POS tags) given the observed sequence of words. It efficiently calculates the probability of a sequence of states by considering both emission and transition probabilities. Viterbi decoding helps identify the most probable sequence of POS tags for a given sentence based on the trained HMM parameters.\n",
    "\n",
    "![The 3 Main Questions of HMMs](hmm_questions.png)\n",
    "\n",
    "## Resources\n",
    "\n",
    "For additional details of the working of HMMs, Matrix Initializations and Viterbi Decoding, you can also consult [Chapter 8](https://web.stanford.edu/~jurafsky/slp3/8.pdf) of the SLP3 book as reference.\n",
    "\n",
    "For a more colorful tutorial, you can also refer to this guide [Hidden Markov Models - An interactive illustration](https://nipunbatra.github.io/hmm/)\n",
    "\n",
    "Another hands-on approach to Viterbi Decoding, can be found in this medium article [Intro to the Viterbi Algorithm](https://medium.com/mlearning-ai/intro-to-the-viterbi-algorithm-8f41c3f43cf3) and can be supplemented by the following slide-set  \n",
    "[HMM : Viterbi algorithm -a toy example](https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf) from the UPenn CIS 2620 course.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>\n",
    "\n",
    "For this notebook, in addition to standard libraries i.e. `numpy`, `nltk`, `collections` and `tqdm`, you are permitted to incorporate supplementary libraries, but it is strongly advised to restrict their inclusion to a minimum. However, other HMM toolkits or libraries are strictly prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\sehar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sehar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#Download the dataset and tagset from below:\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS HMM Class\n",
    "The __POS_HMM__ Class contains the following methods:  \n",
    "\n",
    "1. `__init__(self, corpus)`: initializes the __POS_HMM__ and prepares it for the parameter initialization phase, contains:\n",
    "    - a corpus, which is further 85-15 split into training and test sets.\n",
    "    - a list of all the (word, tag) pairs in the training set (all the sentences are concatenated hence sequence is maintained).\n",
    "    - a tuple of unique words, tags in the training set\n",
    "    - a dictionary for mapping words and tags to its unique integer identifier.\n",
    "    - some additional variables to reduce code redundancy in latter parts such as len()\n",
    "    - Transition, Emission and Initial State Probability Matrices which are initialized to Zeros.\n",
    "    - Tag Occurance Probability matrix, which is initialized with tag probabilities i.e. \n",
    "        (count of a single tag / count of all the tags) in the training set similar to a unigram. This is used for assigning tags to new or __unknown words__ by randomly sampling from all the tag probabilities.\n",
    "2. `init_params(self)`: (__To Be Implemented__) initializes the transition, emission and initial state probability matrices via supervised matrix/parameter learning.  \\\n",
    "    __Parts to Complete__:\n",
    "    - a __method__ `word_given_tag(word, tag)`, which takes as input a word and a tag, and __*returns the count of all instances where the word had the assigned tag as its label / count of the tag*__ (i.e. a probability estimate of its occurence).\n",
    "    - a __method__ `next_tag_given_prev_tag(tag2, tag1)`which takes as input two tags, and __*returns the count of all instances where the tag 2 and was preceeded by tag 1 / count of the first tag*__ (also a probability estimate).\n",
    "    - __add code__ for populating the initial state probability matrix. This essentially contains the probabilities of all POS tags occuring at the start of the sentence.\n",
    "    - __add code__ to normalize each rows of the transition, emission and initial state probability (only has one row) matrices.\n",
    "3. `viterbi_decoding(self, sentence)`: (__To Be Implemented__) returns the mostly likely POS Tags for each word/numeral/punctuation in the sentence.\n",
    "4. `evaluation(self, debug = False)`: evalutes the performance of the POS Tagger on the test set and returns the testing accuracy (as a %age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_HMM:\n",
    "    def __init__(self, corpus): #DO NOT MODIFY THIS FUNCTION\n",
    "\n",
    "        #-----------------DO NOT MODIFY ANYTHING BELOW THIS LINE-----------------\n",
    "        self.corpus = corpus\n",
    "        self.train_set, self.test_set = train_test_split( self.corpus, train_size=0.85,random_state = 777)        \n",
    "        \n",
    "        # Extracting Vocabulary and Tags from our training set\n",
    "        self.all_pairs = [(word, tag) for sentence in self.train_set for word, tag in sentence] #List of tuples (word, POS tag) or a flattened list of tuples by concatenating all sentences\n",
    "        self.vocab = tuple(set(word for (word, _) in self.all_pairs))\n",
    "\n",
    "        self.all_tags = [tag for (_, tag) in self.all_pairs] #List of all POS tags in the trainset\n",
    "        self.tags = tuple(set(self.all_tags)) #List of unique POS tags\n",
    "\n",
    "        # Mapping vocab and tags to integers for indexing\n",
    "        self.vocab2index = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.tag2index = {tag: i for i, tag in enumerate(self.tags)}\n",
    "\n",
    "        self.vocab_len = len(self.vocab) #Total Number of Vocab (Unique Words)\n",
    "        self.all_tag_lengths = len(self.all_tags) #Number of tags in the trainset\n",
    "        self.tag_len = len(self.tags) #Number of unique tags\n",
    "\n",
    "        # Initialize transition and emission matrices (Default: Zeros)\n",
    "        self.transition_mat = np.zeros((self.tag_len, self.tag_len))\n",
    "        self.emission_mat = np.zeros((self.tag_len, self.vocab_len))\n",
    "        self.initial_state_prob = np.zeros(self.tag_len)\n",
    "\n",
    "        # Initialize POS Tag occurance probabilities for getting most likely POS Tags for unknown words\n",
    "        self.tag_occur_prob= {} #Dictionary of POS Tag occurance probabilities\n",
    "        all_tag_counts = Counter(self.all_tags)\n",
    "        for tag in self.tags:\n",
    "            self.tag_occur_prob[tag] = all_tag_counts[tag]/self.all_tag_lengths\n",
    "\n",
    "        #-----------------Add additional variables here-----------------\n",
    "\n",
    "    def init_params(self): #Initialize transition and emission matrices via Supervised Learning (Counting Occurences of emissions and transitions observed in the data).\n",
    "        all_pairs_array = np.array(self.all_pairs)\n",
    "        tags_array = np.array(self.all_tags)\n",
    "\n",
    "        #------------------- Space provided for any additional data structures that you may need or any process that you may need to perform-------------------\n",
    "        #Initialize dictionaries to store counts\n",
    "        word_tag_count = {}\n",
    "        tag_count = Counter(self.all_tags)\n",
    "   \n",
    "        # Count occurrences of words with tags\n",
    "        for word, tag in self.all_pairs:\n",
    "            word_tag_count[(word, tag)] = word_tag_count.get((word, tag), 0) + 1\n",
    "\n",
    "        #-----------------DO NOT ADD ANYTHING BELOW THIS LINE-----------------\n",
    "\n",
    "        # def word_given_tag(word, tag): #Complete this function, skeleton and dummy variables have been provided\n",
    "        #     count_tag = self.all_tags.count(tag)\n",
    "        #     count_w_given_tag = self.all_pairs.count((word, tag))\n",
    "        #     return count_w_given_tag/count_tag\n",
    "        \n",
    "        def next_tag_given_prev_tag(tag2, tag1): #Complete this function, skeleton and dummy variables have been provided\n",
    "            count_t1 = self.all_tags.count(tag1)\n",
    "            count_t2_t1 = sum(1 for t1, t2 in zip(self.all_tags, self.all_tags[1:]) if t1 == tag1 and t2 == tag2)\n",
    "            return count_t2_t1/count_t1\n",
    "       \n",
    "            \n",
    "    \n",
    "            \n",
    "        # Compute Transition Matrix\n",
    "        for i, t1 in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Transition Matrix\", mininterval = 10)):\n",
    "            for j, t2 in enumerate(list(self.tag2index.keys())):\n",
    "                self.transition_mat[i, j] = next_tag_given_prev_tag(t2, t1)\n",
    "                 \n",
    "         # Populate emission matrix using dictionaries\n",
    "            for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Emission Matrix\", mininterval=10)):\n",
    "                for j, word in enumerate(list(self.vocab2index.keys())):\n",
    "                    self.emission_mat[i, j] =( word_tag_count.get((word, tag), 0) + 1)/ tag_count[tag] if tag_count[tag] != 0 else 0\n",
    "                    \n",
    "        # # Compute Emission Matrix\n",
    "        # for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Emission Matrix\", mininterval = 10)):\n",
    "        #     for j, word in enumerate(list(self.vocab2index.keys())):\n",
    "        #         self.emission_mat[i, j] = word_given_tag(word, tag)\n",
    "        \n",
    "        #-------------------Add your code here-------------------\n",
    "        \n",
    "        # Compute Initial State Probability\n",
    "                \n",
    "        #The below code may help. You can modify it as per your requirement.\n",
    "      \n",
    "        for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Initial Probability Matrix\", mininterval=10)):\n",
    "            self.initial_state_prob[i] = sum(1 for sentence in self.train_set if sentence[0][1] == tag) / len(self.train_set)\n",
    "\n",
    "        # Normalize matrices i.e. each row sums to 1\n",
    "            \n",
    "                \n",
    "        self.transition_mat = self.transition_mat / self.transition_mat.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Normalize Emission Matrix\n",
    "        self.emission_mat = self.emission_mat / self.emission_mat.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Normalize Initial State Probability\n",
    "        self.initial_state_prob = self.initial_state_prob / self.initial_state_prob.sum()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        \n",
    "        #-----------------DO NOT MODIFY ANYTHING BELOW THIS LINE-----------------\n",
    "\n",
    "\n",
    "\n",
    "    def viterbi_decoding(self, sentence): #Sentence is a list words i.e. [\"Moon\", \"Landing\", \"was\", \"Faked\"]\n",
    "        \n",
    "        pred_pos_sequence = []  # Implement the Viterbi Algorithm to predict the POS tags of the given sentence\n",
    "\n",
    "        #-------------------Add your code here-------------------\n",
    "\n",
    "        # Initialize viterbi matrix and backpointer matrix\n",
    "        viterbi = np.zeros((len(self.tags), len(sentence)))\n",
    "        backpointer = np.zeros((len(self.tags), len(sentence)), dtype=int)\n",
    "\n",
    "        # Initialize first column of viterbi matrix using initial state probabilities and emission probabilities\n",
    "        for i, tag in enumerate(self.tags):\n",
    "            # Use Laplace smoothing for unknown words\n",
    "            emission_prob = self.emission_mat[i, self.vocab2index.get(sentence[0], -1)]\n",
    "            if emission_prob == 0:\n",
    "                emission_prob = 1 / (self.tag_occur_prob[tag] * len(self.vocab))  # Laplace smoothing\n",
    "            viterbi[i, 0] = self.initial_state_prob[i] * emission_prob\n",
    "\n",
    "        # Iterate over each word in the sentence\n",
    "        for t in range(1, len(sentence)):\n",
    "            # Iterate over each possible current tag\n",
    "            for i, current_tag in enumerate(self.tags):\n",
    "                # Calculate the scores for transitioning from each previous tag to the current tag\n",
    "                scores = [viterbi[j, t - 1] * self.transition_mat[i, j] for j in range(len(self.tags))]\n",
    "                # Find the maximum score and update viterbi matrix\n",
    "                max_score = max(scores)\n",
    "                viterbi[i, t] = max_score\n",
    "                # Update backpointer matrix with the index of the tag that gave the maximum score\n",
    "                backpointer[i, t] = scores.index(max_score)\n",
    "\n",
    "        # Backtrack to find the best path\n",
    "        best_path_pointer = np.argmax(viterbi[:, -1])\n",
    "        best_path = [best_path_pointer]\n",
    "        for t in range(len(sentence) - 1, 0, -1):\n",
    "            best_path_pointer = backpointer[best_path_pointer, t]\n",
    "            best_path.insert(0, best_path_pointer)\n",
    "\n",
    "        # Convert indices to tags\n",
    "        pred_pos_sequence = [list(self.tag2index.keys())[i] for i in best_path]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #-----------------DO NOT MODIFY ANYTHING BELOW THIS LINE-----------------\n",
    "\n",
    "        return pred_pos_sequence\n",
    "    \n",
    "   \n",
    "    def evaluation(self, debug = False): #DO NOT MODIFY THIS FUNCTION\n",
    "        # Evaluate the model on the test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred_pos_sequence = None\n",
    "        flattened_test_sentences = [(word,tag) for test_sentence in self.test_set for word, tag in test_sentence]\n",
    "        test_words = [word for word, _ in flattened_test_sentences]\n",
    "        test_pos_tags = [tag for _, tag in flattened_test_sentences]\n",
    "\n",
    "        pred_pos_sequence = self.viterbi_decoding(test_words)\n",
    "        correct = sum(1 for true_pos, pred_pos in zip(test_pos_tags, pred_pos_sequence) if true_pos == pred_pos)\n",
    "        total = len(test_words)\n",
    "\n",
    "        if debug == True:\n",
    "            print(f\"Sentence (first 20 words): {test_words[:20]}\\n\")\n",
    "            print(f\"True POS Tags (first 20 words):      {test_pos_tags[:20]}\\n\")\n",
    "            print(f\"Predicted POS Tags (first 20 words): {pred_pos_sequence[:20]}\\n\")\n",
    "            # print(f'Correct {correct}  total: {total}')\n",
    "        accuracy = correct / total if total > 0 else 1\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Populating Transition Matrix:   0%|                                                             | 0/12 [00:00<?, ?it/s]\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 23.55it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 22.75it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 27.41it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 32.47it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 12.14it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 23.64it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Populating Transition Matrix:  67%|███████████████████████████████████▎                 | 8/12 [00:11<00:05,  1.39s/it]\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 30.97it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 22.15it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 29.71it/s]\u001b[A\n",
      "\n",
      "Populating Emission Matrix: 100%|██████████████████████████████████████████████████████| 12/12 [00:00<00:00, 20.73it/s]\u001b[A\n",
      "Populating Transition Matrix: 100%|████████████████████████████████████████████████████| 12/12 [00:16<00:00,  1.34s/it]\n",
      "Populating Initial Probability Matrix: 100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 199.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 3.7112%\n"
     ]
    }
   ],
   "source": [
    "pos_hmm = POS_HMM(corpus = conll2000.tagged_sents(tagset='universal'))\n",
    "pos_hmm.init_params()\n",
    "pos_hmm.evaluation(debug = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
